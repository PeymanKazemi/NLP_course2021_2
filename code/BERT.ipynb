{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Rs1l0RWJdRbd",
        "RaYkaoIKwOaa",
        "teEwQqVmwVJ1",
        "8mdYff4zdRby",
        "VwMqoRIeZZQg",
        "1o1nQc2BZ1gJ",
        "MbUEV5OAexrM",
        "1084-AMkhLOs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "c3801f02b1ab900fae6cec7bf788b9edbd67be0743016df220f8b4c20d12b78d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_toBSM-lvh"
      },
      "source": [
        "## Preparation of the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs1l0RWJdRbd"
      },
      "source": [
        "### Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3N8ANd7-lA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e904fe35-18d1-4b62-bbe4-0ccb93940bd8"
      },
      "source": [
        "#  Installation of the following additional packages\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 558 kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.0 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.9)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oU1v7RNFM4b",
        "outputId": "012ca2e1-b21c-45ff-e11d-1e3e53f875a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8655f165513d4ae5ad95914510ad704e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b028244821c74b46a7deb67f6b4d0908",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55099311655741f194c1526a0a92706d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25829c4f5089458ea8685b4ccb5310b8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer([\"Hello, I'm a single sentence! Thist great.\",\"I linke it. At the most.\"],\n",
        "                          [\"test bacht2. test\", \"test2. test\"])\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3-NaLURFPnU",
        "outputId": "4351e8b7-e116-4742-db3f-b027c1da9f75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 1188, 1204, 1632, 119, 102, 2774, 171, 7291, 1204, 1477, 119, 2774, 102], [101, 146, 5088, 1162, 1122, 119, 1335, 1103, 1211, 119, 102, 2774, 1477, 119, 2774, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4XxufpYuFwhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foi5GY9tFhNM",
        "outputId": "71789f6e-76ff-450f-fb71-07360153bd56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS] Hello, I'm a single sentence! Thist great. [SEP] I linke it. At the most. [SEP]\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LALf-IWfdRbh"
      },
      "source": [
        "### Local Installation\n",
        "On a local computer a virtual environment with all needed packages has to be setup. Follow the instructions given on Huggingface [here](https://huggingface.co/course/chapter0?fw=pt)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaYkaoIKwOaa"
      },
      "source": [
        "## Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXMNIp_wwYJ4"
      },
      "source": [
        "### Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ9YO5YjweUb",
        "outputId": "47d73782-129d-48f6-fde5-5a54836cd291"
      },
      "source": [
        "# Import from GoogleDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UttyyLWE1NN-"
      },
      "source": [
        "import os\n",
        "os.chdir(\"//content/gdrive/MyDrive/NLP-Paper/data\")\n",
        "\n",
        "import numpy\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"data.csv\", encoding=\"UTF-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teEwQqVmwVJ1"
      },
      "source": [
        "### Local Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YlrlZzMwR8V"
      },
      "source": [
        "import os\n",
        "os.chdir(\"<Insert the path to your local folder including the data here.>\")\n",
        "\n",
        "import numpy\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"data.csv\", encoding=\"UTF-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mdYff4zdRby"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFJCf4PO3Ye2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9834c0ed-18f4-42c1-f0da-36c99168fd15"
      },
      "source": [
        "# Split the data into two pieces, one for training and one for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_text_series , test_text_series, train_label_series, test_label_series = train_test_split(data[\"text\"], data[\"label\"], test_size = 0.30, random_state = 42)\n",
        "\n",
        "# Split the training data set again to additionally get a validation data set for tuning the hyper parameters of the model\n",
        "#train_text_series, valid_text_series, train_label_series, valid_label_series = train_test_split(train_text_series, train_label_series, test_size = 0.177, random_state = 42)\n",
        "\n",
        "# Casting the data from series objects into lists (as expected from the tokenizer function below)\n",
        "train_text = train_text_series.to_list()\n",
        "#valid_text = valid_text_series.to_list()\n",
        "test_text = test_text_series.to_list()\n",
        "train_label = train_label_series.to_list()\n",
        "#valid_label = valid_label_series.to_list()\n",
        "test_label = test_label_series.to_list()\n",
        "\n",
        "# Sample sizes\n",
        "print(\"Size of the training dataset: \", len(train_text))\n",
        "#print(\"Size of the validation dataset: \", len(valid_text))\n",
        "print(\"Size of the test dataset: \", len(test_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the training dataset:  1461\n",
            "Size of the test dataset:  627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwMqoRIeZZQg"
      },
      "source": [
        "## Tokenizing of the Texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghZvNdJR6lV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1446b4-3b24-4679-e953-eee3a0e30619"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Definition of the model that will be fine-tuned\n",
        "#checkpoint = \"bert-base-german-cased\"\n",
        "checkpoint = \"deepset/gbert-base\"\n",
        "# Getting the tokenizer for the defined model\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Getting the encodings (as tensors for tensorflow) for the texts for training, validation, and testing\n",
        "train_encodings = dict(tokenizer(train_text, padding=True, truncation=True, return_tensors='np'))\n",
        "#valid_encodings = dict(tokenizer(valid_text, padding=True, truncation=True, return_tensors='np'))\n",
        "test_encodings = dict(tokenizer(test_text, padding=True, truncation=True, return_tensors='np'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d9f36c981264a1d81be447581fb4e5b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/83.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75fb3719c4f34012aec44338198b646a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/362 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c41be943e6141a9a3f44b92efa3d87c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/234k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQLrWn3_LZNR",
        "outputId": "4a7328fa-ee89-4f92-b3d5-1429a14c095c"
      },
      "source": [
        "type(tokenizer(train_text, padding=True, truncation=True, return_tensors='np'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.tokenization_utils_base.BatchEncoding"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o1nQc2BZ1gJ"
      },
      "source": [
        "## Class Weight Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDQd4DhZe9fQ",
        "outputId": "2a8e64a3-44bd-4d5f-b669-c59bd28d18e2"
      },
      "source": [
        "# Calculation of class weights to account for the unbalanced sizes of the classes\n",
        "\n",
        "unique, counts = numpy.unique(train_label, return_counts=True)\n",
        "print(\"Class Frequencies: \", dict(zip(unique, counts)))\n",
        "\n",
        "class_weight = {0: counts[1]/counts[0], 1: 1.0}\n",
        "print(\"Class Weights: \", class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Frequencies:  {0: 100, 1: 1361}\n",
            "Class Weights:  {0: 13.61, 1: 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbUEV5OAexrM"
      },
      "source": [
        "## Fine-Tuning with learning rate optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lQrkQu-sYV-"
      },
      "source": [
        "### Definition of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnYjZqwkbas2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed87547-d4ab-43c7-a950-26a982b5a7c7"
      },
      "source": [
        "# Import of all needed functions and packages\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from utils import F1_metric\n",
        "\n",
        "# Definition of batch size and number of epochs\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "\n",
        "# Definition of the learning rate scheduler\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied by the total number of epochs\n",
        "num_train_steps = (len(train_label) // batch_size) * num_epochs\n",
        "lr_scheduler = PolynomialDecay(initial_learning_rate=5e-5, end_learning_rate=0., decay_steps=num_train_steps)\n",
        "\n",
        "# Definition of the optimizer using the learning rate scheduler\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "# Definition of the model architecture and initial weights\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "# Definition of the loss function\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Definition of the full model for training (or fine-tuning)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "179f6cf741c24450a1ea58abf0ea4389",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/515M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFOU2T94Q7zn",
        "outputId": "d5a72553-5494-4440-b2d5-d0f78977f1ea"
      },
      "source": [
        "num_train_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "546"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnOGH_CEBSqO",
        "outputId": "7a16faa3-6f70-4f7c-caad-634ed91b392a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109927680 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,929,218\n",
            "Trainable params: 1,538\n",
            "Non-trainable params: 109,927,680\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7SS1boPC2Z3"
      },
      "source": [
        "model.layers[0].trainable=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3jpXdhqsdn2"
      },
      "source": [
        "### Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K2ucH28smWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbb78fa-f433-410e-d811-18ac5b40dbf8"
      },
      "source": [
        "model.fit(\n",
        "    train_encodings,\n",
        "    np.array(train_label),\n",
        "    #validation_data=(valid_encodings, np.array(valid_label)),\n",
        "    class_weight=class_weight,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fbf40764f30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fbf40764f30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fbf5b917950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fbf5b917950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "183/183 [==============================] - 342s 2s/step - loss: 1.0734 - accuracy: 0.7502\n",
            "Epoch 2/3\n",
            "183/183 [==============================] - 287s 2s/step - loss: 0.5276 - accuracy: 0.9069\n",
            "Epoch 3/3\n",
            "183/183 [==============================] - 289s 2s/step - loss: 0.1738 - accuracy: 0.9706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbe41948b50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1084-AMkhLOs"
      },
      "source": [
        "## Saving and Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Na_LvB65Las"
      },
      "source": [
        "# After fine-tuning you might want to save the model to re-use it later\n",
        "model.save_pretrained(\"hf_model_a4s_i2b.tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Xq9f7A2eBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982815c9-ac7b-4e50-ab76-6be179bbfd5c"
      },
      "source": [
        "# To load an already fine-tuned model to directly use it\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"hf_model_a4s_i2b.tf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at hf_model_a4s_i2b.tf were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at hf_model_a4s_i2b.tf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhXlnofZe3RR"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkrXbSWie7eB"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Calculation of the probabilities for each class\n",
        "# There is no softmax layer at the top of the models in Hugging Face, therefore\n",
        "# the probabilities have to be calculated here using the softmax function\n",
        "test_pred_prob = tf.nn.softmax(model.predict(dict(test_encodings))['logits'])\n",
        "\n",
        "# Extraction of the respective class number with the highest probability\n",
        "test_pred_class = np.argmax(test_pred_prob, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyLpjk09YLjX",
        "outputId": "0fae33a8-7fc8-405d-c628-13edcfc97823"
      },
      "source": [
        "# Checking the test data results\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "# Mean accuracy\n",
        "print(\"Mean Accuracy:\\n\", metrics.accuracy_score(test_label, test_pred_class))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(test_label, test_pred_class))\n",
        "\n",
        "# F1 Score\n",
        "print(\"F1 Score:\\n\", metrics.f1_score(test_label, test_pred_class))\n",
        "\n",
        "# Precision\n",
        "print(\"Precision:\\n\", metrics.precision_score(test_label, test_pred_class))\n",
        "\n",
        "# Recall\n",
        "print(\"Recall:\\n\", metrics.recall_score(test_label, test_pred_class))\n",
        "\n",
        "# ROC AUC Score\n",
        "print(\"ROC AUC:\\n\", metrics.roc_auc_score(test_label, test_pred_class))\n",
        "\n",
        "# Cohen's Kappa Score\n",
        "print(\"Cohen's Kappa:\\n\", metrics.cohen_kappa_score(test_label, test_pred_class))\n",
        "\n",
        "# Quadratic Weighted Kappa Score\n",
        "print(\"Quadratic Weighted Kappa:\\n\", metrics.cohen_kappa_score(test_label, test_pred_class,weights='quadratic'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy:\n",
            " 0.9409888357256778\n",
            "Confusion Matrix:\n",
            " [[ 31  15]\n",
            " [ 22 559]]\n",
            "F1 Score:\n",
            " 0.967965367965368\n",
            "Precision:\n",
            " 0.9738675958188153\n",
            "Recall:\n",
            " 0.9621342512908778\n",
            "ROC AUC:\n",
            " 0.8180236473845693\n",
            "Cohen's Kappa:\n",
            " 0.5944018042904349\n",
            "Quadratic Weighted Kappa:\n",
            " 0.5944018042904349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur0JXbw5eR8r"
      },
      "source": [
        "Checking the probabailty distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTnakAxN9Q4N",
        "outputId": "56f42e20-6550-44a8-9e8a-65925d3ea87b"
      },
      "source": [
        "# Number of answers classified with >95% or <1%\n",
        "high_probs = np.logical_or(np.asarray(test_pred_prob)[:,0]>=.95,np.asarray(test_pred_prob)[:,0]<=.05)\n",
        "unique, counts = numpy.unique(high_probs, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{False: 73, True: 554}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejKXr3_teZ7H",
        "outputId": "49e2ff2a-10cc-4673-88f8-a0f702f9260b"
      },
      "source": [
        "np.histogram(test_pred_prob[:,0])\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#plt.hist(test_pred_prob, bins = 10)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([550,  10,   5,   4,   5,   8,   2,   2,   6,  35]),\n",
              " array([0.00512488, 0.10398107, 0.20283726, 0.30169344, 0.40054962,\n",
              "        0.4994058 , 0.598262  , 0.69711816, 0.7959744 , 0.8948305 ,\n",
              "        0.99368674], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXyeoq8of6fJ",
        "outputId": "3d16be56-28ae-4b1f-e5df-1a6375a5d461"
      },
      "source": [
        "probs_misclassifieds = test_pred_prob[:,0][(test_label-test_pred_class)!=0]\n",
        "print(np.histogram(probs_misclassifieds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 9,  0,  1,  2,  3,  6,  2,  1,  3, 10]), array([0.00720708, 0.10563098, 0.20405486, 0.30247876, 0.40090266,\n",
            "       0.49932656, 0.5977504 , 0.6961743 , 0.7945982 , 0.8930221 ,\n",
            "       0.991446  ], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyErkaj9ijBI",
        "outputId": "2713c0df-cb18-478e-e729-df1566139ea3"
      },
      "source": [
        "np.asarray(test_text)[np.logical_and(np.asarray(test_label)==0, test_pred_class==1)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Hallo Herr Neumann,\\n\\nleider muss ich Ihnen mitteilen, dass Sie mich mit der von Ihnen weitergeleiteten Aufgabe maßlos überfordert haben und ich somit nicht in der Lage bin, diese zu erfüllen. Für die Zukunft sollten sie eventuell auf kurzfristig geplante Meetings verzichten und die Aufgaben selbst erledigen, anstatt diese an Leien wie mich zu übertragen. Es erübrigt sich wohl zu sagen, dass Sie keine Entscheidungsvorlage noch während Ihres Meetings von mir erwarten können.\\n\\nMit freundlichen Grüßen und weiterhin ein wundervolles Meeting\\n\\nLasmiranda Sarantakos\\n\\n',\n",
              "       'Hallo Herr Neumann,\\n\\ngerne würde ich Ihnen meine bearbeiteten Tabellenblätter zusenden, da ich jedoch finde, dass das alles sehr unübersichtlich ist, konnte ich diese nicht bearbeiten und habe deshalb auch keinen Lösungsvorschlag für Sie. Bitte suchen Sie sich einen anderen Depp, der Ihre Drecksarbeit machen soll!\\n\\n\\nMit freundlichen Grüßen \\n\\nEstefania Baumann',\n",
              "       'Guten Tag Frau Meier,\\n\\nich bin mit den Funktionen Ihrer Excel Anwendung nicht zufrieden, falls Sie diese verbessern können Sie sich gerne wieder an mich wenden. Ich habe einen Lösungsvorschlag eingegeben. Diese müsste man nur noch auf die unteren Zellen zu erweitern. Dann können Sie sich diese Ergebnisse / Abweichungen anzeigen lassen. Die Ursachen lassen sich dann bestimmt aus dem Sachverhalt ziehen.\\n\\nMoin Moin und Gruß aus Flensburg Philip Baric.\\n\\n\\n',\n",
              "       'Hallo Herr Neumann,\\n\\nder Fremdbezug ist hier der Eigenfertigung klar vorzuziehen. Leider kann ich keine weiteren detaillierten Informationen darüber geben. \\nDer Vorteil des Fremdbezuges besteht klar darin, dass unsere Mitarbeiter in der Fertigung entlastet werden könnten, beispilesweise durch Überstundenabbau. Des Weiteren sind unsere Kapazitäten von 60 Mitarbeitern schon mehr als ausgelastet. Dieses alleine spricht schon deutlich für den Fremdbezug. Lediglich ein kleines Risiko besteht in der Auswahl des Lieferanten. Dieser sollte zuverlässig liefern und außerdem einen guten Preis und sehr gute Qualität anbieten. Im Rahmen der Lieferantenauswahl würde ich Ihnen gerne unterstützend unter die Arme greifen.\\n\\nInsgesamt denke Ich, dass innerhalb einer Zeitspanne von 30 Minuten, diese doch sehr umfangreiche Aufgabe, für einen Mitarbeiter alleine definitiv nicht zu bewältigen ist. Bitte bedenken Sie nächstes mal, ob Sie die Aufgabe nicht an mehrere Mitarbeiter delegieren können. Ich bin gerne bereit weiter zu untersützen, die genaue Vorgehensweise sollten wir jedoch nocheinmal gemeinsam besprechen.\\nDie Zeitspanne des Meetings ist doch sehr straff gesetzt und die Beteiligten können nicht erwarten, dass sie in dieser Zeitspanne eine Antwort erhalten. Das ist mit menschlichen Fähigkeiten nicht zu schaffen, da zu viele differenzierte Einzelheiten beachtet werden müssen.\\n\\nMit freundlichen Grüßen,\\n\\nManfred Kowalski\\n\\n\\n\\n',\n",
              "       'Hallo Frau Meier,\\n\\nin der Kürze der Zeit und ohne Hilfe von anderen Kolllegen konnte ich die Aufgabe leider nicht lösen.\\nIch finde es nicht richtig mir eine solche Aufgabe zu übergeben, da ich ja auch noch nicht lange im Unternehmen bin.\\nIch würde mich gerne mit Ihnen diesbezüglich gerne nochmal zusammensetzen.\\n\\nFreundliche Grüße',\n",
              "       'Hallo Herr Neumann,\\n\\nleider ist mir auf Grund der Tabelle sowie der mir zur Verfügung stehenden PDF Tabellen NICHT ersichtlich, was fremdbezogen werden soll, wie teuer diese sind.\\nAufgrund der der unzählichen Artikelnummern, unserer Produkte und falsche Informationen von dem Anbieter (die Nummern stimmen in keinster Weise überein) kann ich Ihren Auftrag leider nicht bearbeiten.\\nBeim nächsten Mal würde es mir sehr helfen, ein wenig Übersicht bei der Bearbeitung zu haben und nicht mit unzähligen PDF zugemüllt zu werden, von denen mir keine einzige hilft!\\nIch hoffe Sie finden jemanden, der für diese Aufgabe mächtige bzw. gedulig ist.\\nHiermit verbleibe ich mit freundlichen Grüßen\\nIvan\\n',\n",
              "       'Guten Morgen Herr Neumann, \\nvielen Dank für diese Verantwortungsvolle Aufgabe. \\nIch würde von einer Fremdproduktion abraten ! \\n\\nDie Kunden interessieren sich für unsere deutschen Fahrräder, da \\ndeutsche Qualität überwiegend bevorzugt wird.\\n\\nAuf Grund der Montagezeit und der Lieferzeit, sind preisliche Differenzen fast ausgeglichen.\\n\\nEin minimaler Gewinn der sich eventuell mit Kundenverlust ausgleicht. \\n\\nEin erfolgreiches Meeting.\\nMit freundlichen Grüßen \\n\\nJ. Fischer \\n\\n',\n",
              "       'Hallo,\\n\\nich würde den Lieferanten aus Taiwan nehmen. Tschechien hat miserable Qualität und Deutschland ist zu teuer. Siehe Anhang',\n",
              "       'Hallo Herr Neumann,\\n\\nanbei die Nutzwertanalyse. Wir sollten bei unserem Lieferanten bleiben (Flexi-Steel aus Tschechien).\\nKaum Preisunterschiede. Flexi Steel kennen wir.. Gute Sache.\\n\\nMit freundlichen Grüßen \\n\\nMario Kowalski',\n",
              "       'Hallo Herr Neumann,\\n\\nIch muss mich wirklich dafür entschuldigen dass ich diese Aufgabe nicht übernehmen kann.\\nIch verstehe leider nicht, was Sie genau von mir wollen! Zudem hab ich heute leider keine Zeit dafür!\\nDa ich aber bemüht bin, diese Aufgabe fertig zu stellen. Werde ich warten, bis Sie wieder da sind. und die Aufgabe zusammen mit Ihnen bearbeiten!\\n\\nTut mir leid. \\nBis später.!',\n",
              "       'Hallo Frau Meier,\\n\\ntut mir leid Ihre Aufgaben nicht komplett erledigen zu können.\\n\\nZur Zeit befinde ich mich in der Berufsschule und hab leider keine Zeit dieser aufgabe großartig nachzugehen.\\n\\nAußerdem habe ich Ihnen ein PDF angehangen, wonach wir hier für solche Aufgaben deutlich unterbezahlt werden.\\n\\nAußerdem sind wir als IT Systemkaufleute für die interene IT und Netzwerkstruktur zuständig und nicht für buchhalterische Rechnungen\\n\\nBitte beachten Sie dies in Zukunft\\n\\nGruß\\n\\nC. Fischi',\n",
              "       'So etwas habe ich bisher noch nicht erledigt, benötige Hilfe.',\n",
              "       'Hallo Herr Neumann,\\n\\nanhand des Scoring Modells können sSie sehen welcher Lieferat der Beste ist.\\n\\nMfg',\n",
              "       'Guten Tag Herr Kollege,\\nim Anhang befindet sich meine Auswertung.\\nIch habe mIch für die nette Firma aus dem Taiwan entschieden, da sie mich sehr überzeugt hat. \\n\\nMit freundlichen Grüßen \\n\\nUlf Kowalski',\n",
              "       'Hallo Herr Neumann,\\n\\nich habe heute so viele Aufgaben von Ihnen erhalten, dass ich hierzu nicht mehr komme.\\nIm sorry bro :D\\n\\nMit freundlichen Grüßen\\n\\nAimen Sokolow'],\n",
              "      dtype='<U2967')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdTy8i5qkBCc",
        "outputId": "7b521edf-ba39-4d5e-f086-1dee94f55a08"
      },
      "source": [
        "np.asarray(test_pred_prob)[np.logical_and(np.asarray(test_label)==0, test_pred_class==1)][:,0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44072148, 0.08901641, 0.01014469, 0.47309437, 0.39094636,\n",
              "       0.01444176, 0.01678753, 0.06061301, 0.00720708, 0.3016833 ,\n",
              "       0.46901384, 0.37209886, 0.00840459, 0.00870895, 0.0953871 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfvfVD-wrm0j",
        "outputId": "3ac83f6f-4c39-4ad1-e289-22eab3d1e964"
      },
      "source": [
        "np.asarray(test_text)[np.logical_and(np.asarray(test_label)==1, test_pred_class==0)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Ihre Antwort...\\nSehr geehrte Frau Meier,\\n\\nmeine Rechen-Software ist leider nicht ausgfetüftelt genug, dass ich es in der vorbegebenen Zeit schaffen konnte, mit etwas mehr Zeit hätte es besser geklappt.\\n\\nmit freundlichen Grüßen\\nFruce',\n",
              "       'Hallo Susanne,\\n\\nunter gegebenen Umständen kann ich die folgenden Aufgaben nicht bearbeiten. Die Excel-Simulation ist nicht dafür geeignet. Darüber hinaus funktioniert mein Taschenrechner nicht.\\n\\nMit freundlichen Grüßen\\n\\nTom Sokolow',\n",
              "       'Hallo Chef, \\n\\nich bedanke mich zunächst rechtherzlich für die Einführung in dieses sympatische und hilfsbereite Team, sowie in die benutzerfreundliche Software. Im Folgenen finden Sie die Antworten auf Ihre Probleme.\\n\\n1. (siehe Tabellenkalkulation)\\n2. (siehe Tabellenkalkulation)\\n3. \\n\\nMit freundlichen Grüßen\\nPatrick Schulz',\n",
              "       'Hallo Herr Neumann,\\n\\ndie Unübersichtlichkeit der Listen und Arbeitsmappen (System mit inbegriffen), macht es mir nicht möglich ordentlich zu arbeiten!',\n",
              "       'Ihre Antwort...Vielen Dank für Ihre Mitarbeit und entgegengebrachtes Vertrauen!\\nFalls ich nicht weiter komme würde ich mich bei Ihnen melden!!\\n\\nBis später...\\n\\nLuke',\n",
              "       'Hallo,\\n\\nleider ist mir dieses ein-Fenster-System zu unübersichtlich. Des Weiteren hatte ich nur 15 Minuten Pause, was den gesetzlichen Vorgaben widerspricht. Aufgrund Dessen fühle ich mich nicht in der Lage diese Aufgabe ordnungsgemäß zu erfüllen.\\n\\n\\nHochachtungsvoll,\\n\\nKalle',\n",
              "       'Sehr geehrter Herr Neumann, \\n\\nvielen Dank für Ihr Vertrauen. Ich werde die Aufgabe in dem Umfang und in der Zeit nicht fertig stellen können und möchte Ihnen diese auch nicht unüberprüft als eine so wichtige Entscheidungsgrundlage überlassen. Ich habe eine solche Art von Aufgabe in der Berufsschule schon einmal beantwortet, habe darin jedoch keinerlei Routine. Ich möchte Sie entweder bitten, mir dafür mehr Zeit zu geben damit ich nachschlagen und die Unterlagen auswerten kann oder mir jemaden zur Hilfe bereitzustellen. \\n\\nFreundliche Grüße \\n\\nUschi Petersen',\n",
              "       'Hallo,\\n\\nwerde es in der vorgegebenen Zeit nicht schaffen, muss mich erst einmal richtig in die Aufgabe hineinversetzen.\\n\\nBitte um Verständnis.\\n\\nMfG',\n",
              "       'Servus Frau Meier,\\n\\nich habe die Abweichungen berechnet und die relevanten markiert. \\nUrsachen könnten sein, dass die Lieferanten ihre Preis erhöht haben.\\n\\nMit freundlichen Grüßen\\nJannice Kowalski',\n",
              "       'Hallo Herr Neumann,\\nda das System permanent hängt, ich weder Summen bilden kann geschweige denn arbeiten kann ohne kurz vor einem Wutanfall zu sein, ist die Aufgabe unzumutbar.\\nDie Zeit ist zur knapp und durch diese Arbeitsumstände kann ich Ihnen leider kein zufriedenstellendes Ergebniss liefern.\\n\\nMit freundlichen Grüßen,\\n\\nIhre zuverlässige Frau Kowalski',\n",
              "       'Ich weiß leider nicht was und wie ich bei dieser Aufgabe vorgehen soll.\\nTut mir leid. Aber ich kann diese Aufgabe leider nicht lösen!',\n",
              "       'Hat leider nicht geklappt.\\nDanke',\n",
              "       'Hallo Herr Neumann,\\n\\nich finde es unverantwortlich mich als frisch ausgelernte Mitarbeiterin mit so einer Aufgabe zu beschäftigen.\\nDank der schlechten Aufteilung und Darstellung konnte ich die Aufgabe nicht bearbeiten.\\n\\nBitte kümmern Sie sich darum, dass mir jemand zur Hilfe kommt.\\n\\nVielen Dank. \\n\\n',\n",
              "       'Sehr geehrte Frau Meier,\\n\\nBei der Aufgabe die Sie mir stellten, war mir nicht ganz klar was den hier zu erledigen ist.\\n\\nIch habe trotzdem mein best mögliches gegeben und hoffe man kann gemeinsam in einer Gruppe die korrekte und passende Lösung finden bzw. analsieren.\\n\\nIch würde behaupten frohenmundes wir haben uns eindeutig verschätzt.\\n\\nFrohes Schaffen und einen schönes Arbeitstag.\\n\\nGruß\\n\\nGandolfeberhardtluis Özcan',\n",
              "       'Damit haben Sie sich leider an den Falschen gewendet, das entspricht so gar nicht meinem Aufgabenbereich!',\n",
              "       '...tut mir leid, kann diese Aufgabe nicht bearebiten',\n",
              "       'Sehr geehrter Herr Neumann,\\n\\naufgrund dieses misslungenen Programms, für das garantiert viele Leute sehr viel Geld bekommen haben, ist es mir nicht möglich diese Aufgabe entsprechend der Vorstellungen Ihrerseits zu erfüllen. Ich bitte um Verständnis.\\n\\nP.S. @ Produzenten dieser Studie:\\nWenn man sowas macht,sollte man unter Umständen auch daran denken, die Aufgabe in einem Umfeld zu stellen, die ein halbwegs praxisnahes Bearbeiten ermöglicht. Würde auf jeden Fall ein Ersparnis an Nerven implizieren...Daumen runter für so viel Lärm um nichts.',\n",
              "       'Hallo Herr Neumann,\\n\\ndiese Aufgabe ist unter den gegeben Voraussetzungen eine Zumutung! In 30 Minuten absolut nicht machbar!\\nEventuell sollten wir unsere Benutzeroberfläche nochmals überdenken, diese ist nämlich sehr kontrproduktiv, da man immer nur eine Datei öffnen kann.\\n\\nMit freundlichen Grüßen\\nJuri Sokolow',\n",
              "       'Hallo Paddi,\\n\\nwie bei den Aufgaben zuvor ich habe keine Ahnung, wie ich diese bewerkstelligen soll, da ich sowas noch nie gemacht oder gesehen habe. Es gehört anscheinend nicht in mein Ausbildungsfeld.\\n\\nGruß\\nJoschi',\n",
              "       'Ihre Antwort...wir nehmen die Tschehen.\\n\\nBilliger , zuverlässiger und nicht vom Skandal betroffen.\\n\\nGruß\\n\\n',\n",
              "       'Hi,\\n\\nleider hatte ich zu wenig Zeit. Außerdem war der Arbeitsauftrag im',\n",
              "       'Sehr geehrter Herr Neumann,\\n\\nleider kann ich die Aufgabenstellung die Sie mir erteilt haben nicht lösen.............,\\nda ich mich mit solchen Kalkulationen weder auskenne noch anwende.\\n\\nMit freundlichen Grüßen'],\n",
              "      dtype='<U2967')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo2A6B3YsgT6",
        "outputId": "de32ca88-e1d0-4801-cb9d-08dffa9e745c"
      },
      "source": [
        "np.asarray(test_pred_prob)[np.logical_and(np.asarray(test_label)==1, test_pred_class==0)][:,1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2677139 , 0.06880178, 0.47153053, 0.07231123, 0.10027543,\n",
              "       0.01777794, 0.32678762, 0.4532891 , 0.41646996, 0.07535894,\n",
              "       0.44783783, 0.17739207, 0.42404974, 0.19742522, 0.13282327,\n",
              "       0.00980738, 0.01125221, 0.07086422, 0.00855402, 0.02478631,\n",
              "       0.45773366, 0.33481935], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}